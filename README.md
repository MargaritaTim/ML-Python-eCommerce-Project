# ML-Python-eCommerce-Project
Preprocessing and analysis of eCommerce data, using Python, namely Pandas and Numpy, to make predictions of a customer's conversion. 
Note: The following Project description is currently in Hebrew only. Translation to English is soon to come. 


תקציר מנהלים
•	מסד הנותנים שניתן לנו בפרויקט זה, הכיל נתונים בנושא סשנים (Sessions) של משתמשים באתר קניות באינטרנט. כל סשן יכול להסתיים ברכישה או ללא רכישה. הפרויקט עסק בבעיית קלסיפיקציה בינארית, בה סיווגנו רשומות לשתי קטגוריות: 1 סווג עבור משתמש אשר ביצע רכישה ו-0 סווג עבור משתמש שאינו ביצע רכישה. הסתמכנו בפרויקט על 23 פיצ'רים שחלקם ידועים וחלקם אנונימיים. 
•	הפרויקט חולק ל-3 שלבים עיקריים: 1. חקירת הנתונים ועיבודם – בחלק זה חקרנו כל פיצ'ר לעומק וביצענו שינויים מתאימים על כל מסד הנתונים על מנת לתמוך בסיווגנו, כולל הורדת ממדים.
2. בניית והרצת מודלים – בחרנו 4 מודלים שונים שנלמדו בכיתה, והערכנו את ביצועהם.
3. ביצוע החיזוי בעזרת המודל הטוב ביותר – השתמשנו במודל הטוב ביותר שיצרנו, על מנת לחזות את הסיכוי לרכישה על מסד נתונים חדש, בו אין ידע מקדים האם הקונה רכש או לא.
•	הקוד מומש בסביבת Anaconda במחברת ג'ופיטר, ע"י שימוש בספריות אותן הכרנו במהלך הקורס כגון NumPy, Pandas, Seaborn וכו'. לא נעזרנו בספריות וחבילות נוספות.
צעדי הפרויקט
חקירת הנתונים
תחילה, חקרנו את כל 23 העמודות במסד הנתונים: בדקנו כמה ערכים ריקים יש לכל משתנה, את התפלגויות המשתנים (בדגש על בדיקת ערכים קיצוניים) וחיפשנו קורלציות בין המשתנים השונים. קיבלנו מספר סוגים שונים של נתונים בעמודות:
נתונים מספריים – בדידים או רציפים. 
משתנים קטגוריאליים –קטגוריות בינאריות, קטגוריות בולאניות, קטגוריות מילוליות וקטגוריות מספריות.
ראינו שמשתנים רבים אינם מספריים ולכן החלטנו לשנות אותם לערכים מספריים או למשתני דאמי. עשינו זאת על מנת לבדוק את הקורלציות בין המשתנים. 
חלק מהשאלות שעלו מתוך הסתכלות ראשונית על הנתונים היו: 
האם עמודה D, אשר מלאה בערכים חסרים, תורמת לסיווג הרכישה? מתוך מטריצת הקורלציות עולה כי ישנה קורלציה שלילית חזקה של 0.75- עם עמודת הרכישה ולכן עמודה זאת עלולה להיות משמעותית.
מה משמעות העמודות האנונימיות? לדוגמא: עמודה C נראתה ככזאת שמכילה מספרים מוכרים, כגון 404 שהינה שגיאה ידועה המתקבלת כאשר משתמש מנסה לגשת לעמוד אינטרנט שאינו קיים. 
האם עמודת total_duration הינה שילוב מתמטי של העמודות האחרות המציינות זמן שהייה בסוגי דפים מסויימים, ובכך ניתן למלא ערכים חסרים? החלטנו לחקור את הקשר המתמטי הבא:
total_duration = info_page_duration + admin_page_duration + product_page_duration
האם קנה המידה של הנתונים צריך להיות זהה ובסדר גודל דומה? ובנוסף כיצד להתמודד עם תצפיות קיצוניות (Outliers)? 
בהינתן השאלות הללו, חקרנו כל משתנה בנפרד תוך ביצוע ההתאמות הנדרשות להמשך העיבוד.
עיבוד הנתונים לפי עמודות שונות:
:Weekend
שינינו את הערכים הבוליאניים לערכים מספריים בינאריים. מילאנו ערכים חסרים עם הערך שהיווה את מירב התצפיות –0  (0 - מסמל ימים שאינם סופ"ש, 1 - ימים שהינם סופ"ש. הנחה זאת גם הגיונית מבחינה הסתברותית).
Month:
תחילה, שינינו את הערכים המילוליים לערכים מספריים שמייצגים את החודשים. לאחר השוואה עם עמודת הרכישות, ראינו כי ישנם חודשים בעלי התנהגות דומה בהם גם התבצעו יותר רכישות. על מנת להוריד ממדים תוך שמירת הנתונים, החלטנו לקבץ את החודשים לרבעונים, כפי שנהוג ב-e-commerce.
D:
התלבטנו במעשינו עם משתנה זה, כיוון שהעמודה הכילה מעל ל- 90% ערכים חסרים. ניסינו למלא ערכים אלו במספר דרכים: 0, 1 וחציון, אך ראינו כי ישנה הטיה בתוצאות ללא חשיבות בערך אותו בחרנו כדי למלא משתנה זה. לכן, החלטנו להוריד אותו. לאורך העיבוד, הנחנו כי כדאי לנרמל את כל המשתנים לערכים בין 0 ל-1 ע"מ שפיצ'רים מסויימים לא יקבלו חשיבות יתרה או פחותה שאיננה מוצדקת.
Total_duration, Info_page_duration , Admin_page_duration , Product_page_duration:
שינינו את הערכים המילוליים לערכים מספריים. בנוסף, הסקנו כי שלושת העמודות הללו מרכיבות את העמודה total_duration  וכי מידע זה עלול להיות שימושי ע"מ למלא את הערכים החסרים הרבים. 
את הערכים החסרים ב- info_page_duration , admin_page_durationt מילאנו ב-0. לאחר חישוב יסודי, מילאנו גם את הערכים החסרים בעמודת product_page_duration וב- total_duration.
הורדנו outliers שהינם מעל 75% מהתצפיות ב- total_duration. בחלק זה השתמשנו בBoxplot ע"מ לעשות זאת, והנחנו כי מותר להשתמש בתרשים זה על-אף שההתפלגות בבירור איננה התפלגות נורמלית.
Num_of_info_pages, Num_of_admin_pages, Num_of_product_pages:
קיבצנו את נתוני המשתנים הללו, ע"מ להוריד מימדים תוך שמירה על מלוא המידע, באופן הבא:
Total_num_page = num_of_product_pages + num_of_admin_pages + num_of_info_pages.  הנחנו שלסוגי הדפים השונים אין תפקיד ייחודי עבור כל אחד ואחד מהם בניבוי ההסתברות לרכוש, ולכן ניתן לקבץ אותם.
לבסוף, מילאנו ערכים חסרים בעמודה החדשה ב-0 (חציון). החלטנו לקבץ את הנתונים אפילו יותר על מנת להוריד ממדים, תוך התחשבות בקורלציה עם משתנה הרכישה, באופן הבא:
avg_dur_per_page = Total_duration \ Total_num_page ונרמלנו אותו לקנה מידה בין 0 ל-1. 
הנחנו כי אין תצפיות חריגות עבור total_num_page וכי משך ממוצע לעמוד ינבא קנייה בצורה טובה יותר. 
ExitRates, BounceRates:
החלטנו להשאיר רק את BounceRates מכיוון שישנה קורלציה גבוהה בין שניהם (91%), ועבורו יש קורלציה גבוהה יותר עם עמודת הרכישה. נרמלנו אותו לקנה מידה בין 0 ל-1 ומילאנו ערכים חסרים בחציון.
C:
לפי התצפיות הבנו שמדובר ב- HTTP response status code וחילקנו אותן לקטגוריות הבאות: 
100, 200, 202 - Successful responses: הפכנו זאת ל-1
400, 404 - Client error responses: הפכנו זאת ל- 1-
8080 - port:  הפכנו זאת ל- 0
מילאנו את הערכים החסרים ב-1, אשר מהווה את מירב התצפיות במשתנה זה.
User type:
בדומה למשתנה C, חילקנו לקטגוריות: משתמש חדש – הפכנו ל-1, משתנה חוזר – הפכנו ל- 1-, והשאר 0. מילאנו את הערכים החסרים ב-1-, אשר מהווה את מירב התצפיות במשתנה זה.
Internet_browser, Device:
הנחנו כי קיים קשר בין העמודות (מכשירים מסויימים מקושרים לדפדפנים מסויימים), אך לבסוף מצאנו שלא. את עמודת device חילקנו למשתני דאמי: 4 המכשירים הראשונים נשארו כפי שהיו והשאר קובצו למשתנה אחד. באופן דומה, ב- internet_browser קיבצנו את כל הדפדפנים העיקריים ל- 3 משתני דאמי: Safari, Chrome, Edge, והשאר במשתנה אחד שסווג בשם browser
Region:
בדומה ל- device, חילקנו למשתני דאמי:  1, 3 ו-other. הנחנו כי Regions מתחת ל-1,000 כניסות אינם מספיק חשובים ע"מ להוות Region בפני עצמם.
:ID 
למשתנה זה אין חשיבות לניבוי שלנו ועל כן הורדנו אותו.
B:
זוהו משתנה אנונימי אשר לא פיצחנו את משמעותו, אך הנחנו כי הוא מתפלג נורמאלית. מתוך הנחה זאת, מילאנו ערכים חסרים במשתנים רנדומליים מאותה התפלגות. נרמלנו אותו לקנה מידה בין 0 ל-1. הנחנו כי לא קיימות תצפיות חריגות מספיק על מנת להוריד שורות בעמודה זאת.
A:
הנחנו כי משתנה זה הינו מספור העמודים, אבל שימוש במשתנה זה לא שיפר את המודלים שלנו ועל כן הורדנו אותו על מנת להוריד ממדים ולצמצם אפשרות לoverfitting. 
Closeness_to_holiday:
הסקנו כי מרבית הקניות מתבצעות בימים שאינם קרובים לחג, ולכן חילקנו לקטגוריות: יום שאינו חג – 1 והשאר – 1-. מילאנו את הערכים החסרים ב-1, אשר מהווה את מירב התצפיות במשתנה זה.
Page_values:
זהו הערך בעל החשיבות הגדולה ביותר במסד הנתונים ובעל קורלציה גבוהה עם עמודת הלייבל (רכישה). נרמלנו אותו לקנה מידה בין 0 ל-1 ומילאנו את הערכים החסרים ב-0, אשר מהווה את מירב התצפיות במשתנה זה.
לבסוף הורדנו את ,Region_1.0, Region_3.0, Region_other, device_1.0, device_2.0, q_2, chrome_browser  כיוון שהם בעלי קורלציה גבוהה עם משתנים אחרים וקורלציה נמוכה או זניחה עם הלייבל (רכישה), ע"מ להוריד ממדים. הנחנו כי על-אף שעמודות אלו עלולות להיות חשובות,  בכ"ז נעדיף בטרייד-אוף זמן הרצה מהיר יותר וסיכוי נמוך יותר ל- overfitting.
הורדמת מימדים
בנוסך להורדת הממדים שכבר עשינו, ניסינו להעזר ב- Feature Importance – פונקציה הלומדת את הנתונים ובכך מחשבת לכל משתנה כמה מידע אנו מקבלים ממנו לפי Loss Function. התוצאות לא נראו כתורמות במיוחד לניבוי ולכן לא השתמשנו בתוצאות. בנוסף, השתמשנו ב-PCA וראינו כי אין צורך להוריד משתנים נוספים.
בניית המודלים
בחרנו להשתמש במודלים הבאים:   ,Naïve bayes classifier,  Logistic regression, Decision tree Random RandomForest classifier.
: Gaussian NB למודל זה אין היפר פרמטרים, שכן אין לנו ידע קודם על הנתונים על מנת להזין הסתברויות לכל מאפיין.
עבור המודלים האחרים השתמשנו ב- Grid Search על מנת לבחור את ההיפר-פרמטרים הטובים ביותר. חילקנו את מסד הנתונים המעובד ל- train ו- validation. הסתמכנו על מסד ה- train על מנת למצוא את ההיפר פרמטרים הטובים ביותר דרך Grid Search, ובעזרת ה- validation בחנו את התוצאות שלנו. כיוון שיש צורך למקסם את ה- AUC של כל מודל, השתמשנו ב- Grid Search כך שימקסם את AUC עבור הנתונים. בין מודל למודל, בדקנו פרמטרים שונים אשר אנו סבורות כי הם בעלי חשיבות והשפעה למודל הנבדק.
ההיפר פארמטרים שנבחרו הינם:
LogisticRegression-  {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}
DecisionTreeClassifier - {'criterion': 'entropy', 'max_depth': 4, 'max_features': 10, 'min_samples_split': 10, 'splitter': 'best'}
RandomForestClassifier - {'criterion': 'entropy', 'max_depth': 50, 'max_features': 8, 'min_samples_leaf': 50, 'n_estimators': 65}
הערכת המודלים
בניית Matrix Confusion – בנינו את המטריצה עבור המודל RandomForestClassifier. ציר ה- X מייצג את הנעשה בפועל: האם התבצעה רכישה או לא. ציר ה- Y מייצג את התחזיות של מודל: בין אם המודל חזה שתתרחש רכישה ובין אם לא. לכן: הערך התחתון בצד ימין הוא TP : שיעור - מספר הפעמים שהמודל ניבא נכון שהייתה רכישה.                             הערך התחתון בצד שמאל הוא :FP מספר הפעמים שהמודל ניבא שלא שהייתה רכישה באופן שגוי.
הערך העליון בצד ימין הוא :FN מספר הפעמים שהמודל ניבא שלא הייתה רכישה באופן שגוי.
הערך העליון בצד שמאל הוא :TN מספר הפעמים שהמודל ניבא נכון שלא הייתה רכישה. 
נשים לב שהמודל שלנו ניבא יותר פעמים נכון מלא נכון, רק בפחות מ-5.5% ניבאנו לא נכון.
הערכת המודלים נעשתה על ידי שימוש ב- K-Fold, בפונקציה KfoldPlot. הרצנו זאת על סט ה- validation כך הערכנו את המודלים שלנו. ציירנו עקומות ROC וחישבנו את ה-AUC לכל מודל עבור ה- train  ועבור ה- validation. לאור תוצאותינו, קיבלנו כי למודל RandomForestClassifier יש AUC הגבוהה ביותר הן עבור validation והן עבור train.
לאור התוצאות הללו, בחרנו במודל RandomForestClassifier בתור המודל המנבא שלנו.
ביצוע הניבוי
קראנו את קובץ ה-test והרצנו עליו את כל תהליך העיבוד שנעשה על קובץ ה- train.
לאחר מכן ביצענו הרצה עבור מודל RandomForestClassifier פעם נוספת, על כל הערכים מסט ה-train  לאחר העיבוד. לאחר מכן, ביצענו את הניבוי על קובץ ה-test וקיבלנו עבור כל Id של פרט את ההסתברות לבצע רכישה או לא. לבסוף הכנסנו את התוצאות לקובץ אחד. בנוסף, בסוף התוכנית הוספנו pipeline. כלומר, קטע קוד שמבצע בקצרה את כל התהליך הנאמר לעיל.
סיכום
עסקנו בפרויקט בבעיית קלסיפיקציה בינארית בתחום הקניות באינטרנט. ביצענו שימוש בכלים מגוונים ויישמנו את חומר של הקורס באופן מעשי. המטרה העיקרית של הפרויקט הייתה למקסם את ה AUC על מנת לחזות האם בוצעה רכישה, בצורה הטובה ביותר. זאת באמצעות קומבינציות של אופציות הרצה שונות המשלבות מודלים, שיטות נרמול נתונים ועוד. ביצענו בפרויקט חקירה ועיבוד הנתונים מהם למדנו על הנתונים והסקנו מסקנות, טיפלנו בנתונים חסרים וחריגים ופעלנו על מנת להוריד ממדים בטכניקות שונות. כמו כן, נרמלנו את הנתונים והעברנו את המשתנים הקטגוריאליים למשתני דאמי וקטגוריות בינאריות. 
לאחר מכן, בנינו את המודלים והפעלנו אותם על סט הנתונים. לאחר הערכת המודלים באמצעות K-Fold, מצאנו כי המודל שהביא לתוצאת ה- AUC הגבוהה ביותר הינו RandomForestClassifier. החלטנו לבצע את הניבוי באמצעות מודל זה על קובץ test, ושמרנו את התוצאות בקובץ חדש. 
